#!/bin/bash
set -euo pipefail

# Create ipython config directory if it doesn't exist
mkdir -p ${CONDA_DIR}/etc/ipython
cp ipython_config.py ${CONDA_DIR}/etc/ipython/ipython_config.py

# Allow users to stop / start servers from JupyterLab
jupyter labextension install @jupyterlab/hub-extension

# Download Spark
echo "Download & Extract spark without Hadoop"
SPARK_HOME=${CONDA_DIR}/spark
SPARK_VERSION=2.4.2

mkdir -p ${SPARK_HOME}
wget -q https://archive.apache.org/dist/spark/spark-${SPARK_VERSION}/spark-${SPARK_VERSION}-bin-without-hadoop.tgz -O - | \
    tar -xz --strip 1 -C ${SPARK_HOME}

echo "Installing Spark into Python"
# must cd into ${SPARK_HOME}/python before running setup.py. Sigh.
pushd .
cd ${SPARK_HOME}/python
python setup.py install
popd

echo "Downloading Guava and GCS connector"
echo "Downloading Guava"
GUAVA_VERSION=23.0 
wget -q http://central.maven.org/maven2/com/google/guava/guava/${GUAVA_VERSION}/guava-${GUAVA_VERSION}.jar \
    -P ${SPARK_HOME}/jars
echo "Download GCS connector"
# FIXME: Put a version here?
wget -q https://storage.googleapis.com/hadoop-lib/gcs/gcs-connector-latest-hadoop2.jar \
    -P ${SPARK_HOME}/jars

echo "Installing Hadoop"
# install hdfs
HADOOP_HOME=${CONDA_DIR}/hadoop
HADOOP_VERSION=2.7.7

mkdir -p ${HADOOP_HOME}
wget -q http://archive.apache.org/dist/hadoop/core/hadoop-${HADOOP_VERSION}/hadoop-${HADOOP_VERSION}.tar.gz -O - | \
    tar -xz --strip 1 -C ${HADOOP_HOME}

# Add GS as a filesystem to HDFS
cp ${REPO_DIR}/core-site.xml ${HADOOP_HOME}/etc/hadoop/core-site.xml

# Add HDFS configs to Spark
cp ${REPO_DIR}/spark-env.sh ${SPARK_HOME}/conf/spark-env.sh

# Install gcloud and kubectl commands
GCLOUD_HOME=${CONDA_DIR}/gcloud
mkdir -p ${GCLOUD_HOME}
wget -q https://dl.google.com/dl/cloudsdk/channels/rapid/downloads/google-cloud-sdk-245.0.0-linux-x86_64.tar.gz -O - | \
    tar -xz --strip 1 -C ${GCLOUD_HOME}

${GCLOUD_HOME}/bin/gcloud components install kubectl