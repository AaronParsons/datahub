#!/bin/bash
set -euo pipefail

# Create ipython config directory if it doesn't exist
mkdir -p ${CONDA_DIR}/etc/ipython
cp ipython_config.py ${CONDA_DIR}/etc/ipython/ipython_config.py

# Allow users to stop / start servers from JupyterLab
jupyter labextension install @jupyterlab/hub-extension

# Download Spark
echo "Download & Extract spark without Hadoop"
SPARK_HOME=${CONDA_DIR}/spark
SPARK_VERSION=2.4.2

mkdir -p ${SPARK_HOME}
cd ${SPARK_HOME}
wget -q https://archive.apache.org/dist/spark/spark-${SPARK_VERSION}/spark-${SPARK_VERSION}-bin-without-hadoop.tgz -O - | \
    tar -xz --strip 1

echo "Installing Spark into Python"
cd ${SPARK_HOME}/python
python setup.py install

echo "Downloading Guava and GCS connector"
cd ${SPARK_HOME}/jars
echo "Downloading Guava"
GUAVA_VERSION=23.0
wget -q http://central.maven.org/maven2/com/google/guava/guava/${GUAVA_VERSION}/guava-${GUAVA_VERSION}.jar
echo "Download GCS connector"
# FIXME: Put a version here?
curl -q https://storage.googleapis.com/hadoop-lib/gcs/gcs-connector-latest-hadoop2.jar

echo "Installing Hadoop"
# install hdfs
HADOOP_HOME=${CONDA_DIR}/hadoop
HADOOP_VERSION=2.7.7

mkdir -p ${HADOOP_HOME}
cd ${HADOOP_HOME}
wget -q http://archive.apache.org/dist/hadoop/core/hadoop-${HADOOP_VERSION}/hadoop-${HADOOP_VERSION}.tar.gz -O - | \
    tar -xz --strip 1

# Add GS as a filesystem to HDFS
cp core-site.xml ${HADOOP_HOME}/etc/hadoop/core-site.xml

# Add HDFS configs to Spark
cp spark-env.sh ${SPARK_HOME}/conf/spark-env.sh